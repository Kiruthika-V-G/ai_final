HMM.py

start_probabilities = {"H": 0.5, "L": 0.5}

transition_probabilities = {
    "H": {"H": 0.5, "L": 0.5},
    "L": {"H": 0.4, "L": 0.6}
}

emission_probabilities = {
    "H": {"A": 0.2, "C": 0.3, "G": 0.3, "T": 0.2},
    "L": {"A": 0.3, "C": 0.2, "G": 0.2, "T": 0.3}
}

def forward(sequence, states, start_p, transition_p, emit_p):
    forward_probs = {
        state: start_p[state] * emit_p[state][sequence[0]] for state in states
    }

    for i in sequence[1:]:
        new_forward_probs = {}
        for state in states:
            prob = sum(
                forward_probs[prev_state] * transition_p[prev_state][state] for prev_state in states
            )
            new_forward_probs[state] = prob * emit_p[state][i]
        forward_probs = new_forward_probs
    
    return sum(forward_probs[state] for state in states)


def viterbi(sequence, states, start_p, transition_p, emit_p):
    max_forward_probs = {
        state: start_p[state] * emit_p[state][sequence[0]] for state in states
    }

    path =  {
        state : [state] for state in states
    }

    for i in sequence[1:]:
        new_forward_probs = {}
        new_path = {}
        for state in states:
            prob, prev_state = max(
                (max_forward_probs[prev_state] * transition_p[prev_state][state], prev_state) for prev_state in states
            )
            new_forward_probs[state] = prob * emit_p[state][i]
            new_path[state] = path[prev_state] + [state]
        path = new_path
        max_forward_probs = new_forward_probs
    
    max_prob, max_state = max((max_forward_probs[state], state) for state in states)
    probable_sequence = "-->".join(x for x in path[max_state])
    return probable_sequence
    

print(f'{forward("GGCACTGAA", ["H", "L"], start_probabilities, transition_probabilities, emission_probabilities):0.20f}')

print(viterbi("GGCACTGAA", ["H", "L"], start_probabilities,
      transition_probabilities, emission_probabilities))


-----------------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------------

MDP.py

import numpy as np
GAMMA = 0.9

class State:
    def __init__(self, name, reward):
        self.name = name
        self.reward = reward
        self.actions = {}
        self.expected_rewards = []
        self.expected_reward = reward

    def add_transition(self, action, state, probability):
        if action in self.actions:
            self.actions[action].append((state, probability))
        else:
            self.actions[action] = [(state, probability)]
    
    # Called at the end of iteration
    def update_expected_reward(self):
        self.expected_rewards.append(self.expected_reward)
    
    # Called to calculate expected reward
    def calculate_expected_reward(self):
        # Maximum rewarding action
        maximum_reward = -1 * np.inf
        for action in self.actions:
            action_reward = 0
            for state, probability in self.actions[action]:
                action_reward += state.expected_rewards[-1] * probability
            maximum_reward = np.maximum(action_reward, maximum_reward)
        
        expected_reward = self.reward + GAMMA * maximum_reward
        
        if np.isclose(self.expected_reward, expected_reward):
            return True
        else:
            self.expected_reward = expected_reward
            return False


if __name__ == "__main__":
    sun = State("sun", 4)
    wind = State("wind", 0)
    hail = State("hail", -8)
    
    sun.add_transition("a", wind, 0.5)
    sun.add_transition("a", sun, 0.5)
    
    wind.add_transition("a", sun, 0.5)
    wind.add_transition("a", hail, 0.5)
    
    hail.add_transition("a", wind, 0.5)
    hail.add_transition("a", hail, 0.5)
    
    states = [sun, wind, hail]
    shouldStopCheck = [False] * len(states)
    
    print("Iteration", end="\t\t")
    for state in states:
        print(f"{state.name}", end="\t\t")
    print()
    
    i = 0
    while not all(shouldStopCheck):
        for state in states:
            state.update_expected_reward()
        
        print(f"{i}", end="\t\t")
        for state in states:
            print(f"{state.expected_reward: .2f}", end="\t\t")
        print()

        for index, state in enumerate(states):
            shouldStopCheck[index] = state.calculate_expected_reward()
        
        i += 1


------------------------------------------------------------------------

MDP 2

state_valuesi = {'pu':0, 'pf':0, 'ru':10, 'rf':10}
state_values = {'pu':0, 'pf':0, 'ru':10, 'rf':10}
pu={'s':{'pu':1}, 'a':{'pu':0.5, 'pf':0.5}}
pf={'s':{'pu':0.5, 'rf':0.5}, 'a':{'pf':1}}
ru={'s':{'ru':0.5, 'pu':0.5}, 'a':{'pu':0.5, 'pf':0.5}}
rf={'s':{'ru':0.5,'rf':0.5}, 'a':{'pf':1}}
j=0
while(j<5):
    sumS=0
    sumA=0
    for i in pu['s']:
        sumS+=pu['s'][i]*state_values[i]
    for i in pu['a']:
        sumA+=pu['a'][i]*state_values[i]
    puV=state_valuesi['pu']+0.9*max(sumS, sumA)
    
    
    sumS=0
    sumA=0
    for i in pf['s']:
        sumS+=pf['s'][i]*state_values[i]
    for i in pf['a']:
        sumA+=pf['a'][i]*state_values[i]
    pfV= state_valuesi['pf']+0.9*max(sumS, sumA)
    
    
    sumS=0
    sumA=0
    for i in ru['s']:
        sumS+=ru['s'][i]*state_values[i]
    for i in ru['a']:
        sumA+=ru['a'][i]*state_values[i]
    ruV=state_valuesi['ru']+0.9*max(sumS, sumA)
    
    
    sumS=0
    sumA=0
    for i in rf['s']:
        sumS+=rf['s'][i]*state_values[i]
    for i in rf['a']:
        sumA+=rf['a'][i]*state_values[i]
    rfV = state_valuesi['rf']+0.9*max(sumS, sumA)
    
    state_values['pu']=puV
    state_values['pf']=pfV
    state_values['ru']=ruV
    state_values['rf']=rfV
    j+=1
    print(state_values)
